# Dependencies:
import utility as u
import numpy as np
import pandas as pd
import repeat as r

# STEP 1 - Dowload the data:
# See import_data() function in utility module. Bash script:
# sort spambase.data -R -0 spambase.data
# was run first to randomize order of observations in the file.
data = u.import_data()

# STEP 2 - Partition the data into 10 folds:
# custom_idx_ranges(data) returns a dictionary, with integers from 0 to 9 as fold keys
# and with associated values being a range of indices in 10 step increaments.
# split_data(data, subsample_indices, excluded_subsample) function returns training and validation datasets for a given
# fold iteration.

# STEP 3 - Normalize observations:
# normalize(data) - scales observations to 0 mean and unit variance
data.iloc[:, 0:57] = u.normalize(data.iloc[:, 0:57])

# STEP 4 - Linear regression learner vis SGD:
# shuffle_index(data) function in the utility module generates a shuffled list of indexes of the size
# equal to number of observations in the data. At each epoch this function is called to generate new order in which
# observations are shown to the SGD trainer.
# get_batch(data, indices, size, i) function accepts: the dataset, ordered by "indices" as generated by shuffle_index(),
# required batch "size" and index of the batch to retrieve.
# sgd(data, loss=lr_mse_loss, gradient_loss=lr_gradient_loss, alpha=0.001, max_epoch=10, convergence=0.1, batch_size=1)
#   -set all parameters to 0
#   -continue until max_epoch is reached:
#       -shuffle order of observations
#       -continue for all batches from the dataset:
#           -obtain next batch of data of "batch_size" length
#           -compute gradient of loss function for this batch
#           -update parameters
#       -calculate and save loss at the whole dataset
#
# cv(raw_data, ...):
#   applies sgd on folds generated by custom_idx_ranges(raw_data) and records performance on the validation set

config = u.config

# STEP 5:
# default option is to use sgd with batch size = 1 and linear regression loss
# alphas = [0.001, 0.005, 0.0001]
# cv_mean_loss = {}
# dict_of_losses = {}
# for alpha in alphas:
#     config['alpha'] = alpha
#     train_loss, test_loss, cv_losses_over_epochs = u.cv(raw_data=data, c=config)
#     dict_of_losses[str(alpha)] = cv_losses_over_epochs
#     avg_test_loss = np.mean(test_loss)
#     cv_mean_loss[str(alpha)] = avg_test_loss
#
# print(cv_mean_loss)
# best_alpha = min(cv_mean_loss, key=cv_mean_loss.get)
# print("The best alpha is: " + str(best_alpha))
# u.draw_losses(dict_of_losses=dict_of_losses)
#
# # STEP 6:
#
# mean_fpr, mean_tpr = u.collect_cv_results(data, config)
# u.draw_roc(mean_fpr, mean_tpr)

# STEP 7: BATCH GRADIENT DESCENT
# config['batch_size'] = 100
# r.repeat4to7(data, config)
#
# # STEP 8: LOGISTIC REGRESSION
config['gradient_loss'] = u.log_loss
config['batch_size'] = 1
r.repeat4to7(data, config)
# config['batch_size'] = data.shape[0]
# r.repeat4to7(data, config)
