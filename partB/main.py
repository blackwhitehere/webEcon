# Module import:
import utility as u
import repeat as r

# utility module implements function called from the main
# repeat module implements steps 4 to 6 for different configurations of sgd
# different loss function, alpha, batch_size, max_epochs etc be specified (see u.config for details)

# STEP 1 - Download the data:
# See u.import_data() function in the utility module.
# Modify path to data if need be.
# Bash script:
# " sort spambase.data -R -0 spambase.data "
# was run first to randomize order of observations in the file.
data = u.import_data()

# STEP 2 - Partition the data into 10 folds:
# -u.custom_idx_ranges(data) returns a dictionary, with integers from 0 to 9 as fold keys.
# The associated values are ranges of indices in 10 step increments.
# -u.split_data(data, subsample_indices, excluded_subsample) function returns training and validation datasets
# for a given fold iteration.

# STEP 3 - Normalize observations:
# normalize(data) - scales observations to 0 mean and unit variance. Since data is split into observations
# and response in the utility module, the observations are manually normalized here.
data.iloc[:, 0:57] = u.normalize(data.iloc[:, 0:57])

# STEP 4 - Linear regression learner vis SGD:
# -shuffle_index(data) function in the utility module generates a shuffled list of indexes of the size
# equal to number of observations in the data. At each epoch this function is called to generate new order in which
# observations are shown to the SGD trainer.
# -get_batch(data, indices, size, i) function accepts: the dataset, reordered "indices",
# as generated by shuffle_index(), required batch "size" and index position of the batch to retrieve.
# -sgd(data, loss=lr_mse_loss, gradient_loss=lr_gradient_loss, alpha=0.001, max_epoch=10, convergence=0.1, batch_size=1)
#   -set all parameters to 0
#   -continue until max_epoch is reached or fall in loss is minimal:
#       -shuffle order of observations
#       -continue for all batches from the dataset:
#           -obtain next batch of data of "batch_size" length
#           -compute gradient of loss function for this batch
#           -update parameters
#       -calculate and save loss at the whole dataset
#
# cv(raw_data, ...):
#   applies sgd on folds generated by custom_idx_ranges(raw_data) and records performance on the validation set

config = u.config

# STEP 4-6:
# default option in config is to use sgd with batch size = 1 and linear regression loss
msg = "Showing output for Stochastic Linear Regression"
config['msg'] = msg
r.repeat4to6(data, config)

# STEP 7: BATCH GRADIENT DESCENT
config['batch_size'] = 920
msg = "Showing output for Batch Linear Regression"
config['msg'] = msg
r.repeat4to6(data, config)
#
# STEP 8: LOGISTIC REGRESSION
# STOCHASTIC, BATCH_SIZE = 1
config['loss'] = u.log_loss
config['batch_size'] = 1
msg = "Showing output for Stochastic Logistic Regression"
config['msg'] = msg
r.repeat4to6(data, config)

# BATCH:
config['batch_size'] = 920
msg = "Showing output for Batch Logistic Regression"
config['msg'] = msg
r.repeat4to6(data, config)
